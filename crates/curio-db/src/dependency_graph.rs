//! # Dependency Graph Library
//!
//! This library implements a build dependency graph using DynamoDB Single Table Design.
//! It supports "Immutable Compute Nodes" where a node's identity is derived from its inputs.
//!
//! ## Data Model
//!
//! ### Entities
//! *   **Artifact (`ARTIFACT#{Checksum}`)**: Represents a file or data object.
//!     *   `is_external`: Boolean. If true, managed by user (source files). If false, generated by compute (build outputs).
//! *   **Compute Node (`COMPUTE#{Id}`)**: Represents a transformation step.
//!     *   `inputs`: List of Artifact IDs.
//!     *   `outputs`: List of Artifact IDs.
//!     *   `type`: String (e.g., "Compile", "Link").
//!
//! ### Edges (Reverse Index)
//! To efficiently answer "Who depends on Artifact A?", we store Edge items:
//! *   **PK**: `ARTIFACT#{A}`, **SK**: `COMPUTE#{C}`.
//!
//! ## Workflows
//!
//! ### 1. New Build Flow
//! ```text
//! [Register A] -> [Create C (Inputs: A)] -> [Run C] -> [Set Outputs: B]
//!      |                  |                                   |
//!   (Artifact A)      (Compute C IS DIRTY)               (Artifact B, C IS CLEAN)
//! ```
//!
//! ### 2. Update Flow (Input Change `A` -> `A'`)
//! Triggered when an external artifact changes.
//!
//! 1.  **Register New**: User registers `A'` (new checksum).
//! 2.  **Identify Downstream**: Query `get_downstream_compute_nodes(A)` -> Found `C_old`.
//! 3.  **Reconstruct**: Get details of `C_old` (Type="Compile", Inputs=[A]).
//! 4.  **Remove Old**: `remove_compute_node(C_old)`.
//!     *   Marks `C_old` outputs (e.g., `B`) as `STATUS#ORPHAN`.
//! 5.  **Create New**: User creates `C_new` with inputs `[A']`.
//!     *   `C_new` is DIRTY.
//! 6.  **Run & Output**: `C_new` runs, produces `B'`.
//!     *   `set_compute_node_outputs(C_new, [B'])`.
//!
//! ### 3. Garbage Collection (GC)
//! Artifacts are reference-counted implicitly by the graph structure.
//!
//! *   **Orphan Creation**:
//!     *   External: `mark_artifact_orphaned(ID)`.
//!     *   Internal: When a Compute Node is removed, its outputs become ORPHANS.
//!     *   Internal: When `set_compute_node_outputs` changes outputs, old ones become ORPHANS.
//! *   **Cleanup (`cleanup_orphans`)**:
//!     *   Scans `STATUS#ORPHAN` index.
//!     *   **Safe Check**: For each orphan, verifies `count(downstream_edges) == 0`.
//!     *   If safe, DELETE artifact.
//!
//! ## DynamoDB Schema
//! | Entity       |     PK               | SK             | GSI1PK         | GSI1SK           | Notes               |
//! |--------------|----------------------|----------------|----------------|------------------|---------------------|
//! | **Artifact** | `ARTIFACT#{Sum}`     | `META`         | `STATUS#ORPHAN`| `ARTIFACT#{Sum}` | Meta info           |
//! | **Compute**  | `COMPUTE#{Id}`       | `META`         | `STATUS#DIRTY` | `COMPUTE#{Id}`   | Inputs/Outputs list |
//! | **Edge**     | `ARTIFACT#{Sum}`     | `COMPUTE#{Id}` | -              | -                | Reverse lookup      |
//!

use aws_sdk_dynamodb::{
    types::{AttributeValue, TransactWriteItem, Put},
    Client,
};
use futures::stream::Stream;
use std::pin::Pin;

pub type ArtifactId = String;
pub type ComputeNodeId = String;

pub struct DependencyGraph {
    client: Client,
    table_name: String,
}

impl DependencyGraph {
    pub fn new(client: Client, table_name: String) -> Self {
        Self { client, table_name }
    }

    fn pk_artifact(id: &str) -> String { format!("ARTIFACT#{}", id) }
    fn pk_compute(id: &str) -> String { format!("COMPUTE#{}", id) }
    fn sk_meta() -> String { "META".to_string() }
    fn sk_edge_compute(id: &str) -> String { format!("COMPUTE#{}", id) }
    
    // GSI1
    fn gsi1_dirty_pk() -> String { "STATUS#DIRTY".to_string() }
    fn gsi1_orphan_pk() -> String { "STATUS#ORPHAN".to_string() }
    fn gsi1_compute_sk(id: &str) -> String { format!("COMPUTE#{}", id) }
    fn gsi1_artifact_sk(id: &str) -> String { format!("ARTIFACT#{}", id) }

    /// Registers an artifact existence. ID is the checksum.
    pub async fn register_artifact(&self, id: ArtifactId, is_external: bool) -> Result<(), aws_sdk_dynamodb::Error> {
        self.client.put_item()
            .table_name(&self.table_name)
            .item("pk", AttributeValue::S(Self::pk_artifact(&id)))
            .item("sk", AttributeValue::S(Self::sk_meta()))
            .item("created_at", AttributeValue::S(chrono::Utc::now().to_rfc3339()))
            .item("is_external", AttributeValue::Bool(is_external))
            .send()
            .await?;
        Ok(())
    }

    /// Explicitly marks an artifact as STATUS#ORPHAN.
    pub async fn mark_artifact_orphaned(&self, id: ArtifactId) -> Result<(), aws_sdk_dynamodb::Error> {
        self.client.update_item()
            .table_name(&self.table_name)
            .key("pk", AttributeValue::S(Self::pk_artifact(&id)))
            .key("sk", AttributeValue::S(Self::sk_meta()))
            .update_expression("SET gsi1pk = :pk, gsi1sk = :sk")
            .expression_attribute_values(":pk", AttributeValue::S(Self::gsi1_orphan_pk()))
            .expression_attribute_values(":sk", AttributeValue::S(Self::gsi1_artifact_sk(&id)))
            .send()
            .await?;
        Ok(())
    }

    /// Creates a new compute node.
    /// ID is derived by caller.
    /// Marks as is_dirty = true initially.
    pub async fn create_compute_node(&self, id: ComputeNodeId, inputs: Vec<ArtifactId>, node_type: String) -> Result<(), aws_sdk_dynamodb::Error> {
        let mut transaction = Vec::new();

        // 1. Put Compute Node (Meta + Dirty Status)
        let mut compute_put = Put::builder()
            .table_name(&self.table_name)
            .item("pk", AttributeValue::S(Self::pk_compute(&id)))
            .item("sk", AttributeValue::S(Self::sk_meta()))
            .item("gsi1pk", AttributeValue::S(Self::gsi1_dirty_pk())) // Mark Dirty
            .item("gsi1sk", AttributeValue::S(Self::gsi1_compute_sk(&id)))
            .item("type", AttributeValue::S(node_type));

        let input_attrs: Vec<AttributeValue> = inputs.iter().map(|i| AttributeValue::S(i.clone())).collect();
        compute_put = compute_put.item("inputs", AttributeValue::L(input_attrs));

        transaction.push(TransactWriteItem::builder().put(compute_put.build().unwrap()).build());

        // 2. Put Edges (Reverse Index: Artifact -> ComputeNode)
        for input_id in &inputs {
            let edge_put = Put::builder()
                .table_name(&self.table_name)
                .item("pk", AttributeValue::S(Self::pk_artifact(input_id)))
                .item("sk", AttributeValue::S(Self::sk_edge_compute(&id)))
                .build()
                .unwrap();
            transaction.push(TransactWriteItem::builder().put(edge_put).build());
        }

        self.client.transact_write_items()
            .set_transact_items(Some(transaction))
            .send()
            .await?;

        Ok(())
    }

    /// Updates the node's sorted outputs.
    /// Side Effect: Sets is_dirty = false (marks as clean).
    /// Marks old outputs as STATUS#ORPHAN.
    pub async fn set_compute_node_outputs(&self, id: ComputeNodeId, outputs: Vec<ArtifactId>) -> Result<(), aws_sdk_dynamodb::Error> {
        // 1. Get current outputs to identify orphans
        let (_, _old_outputs) = self.get_compute_node_details(id.clone()).await.unwrap_or_default();
        let (_, old_actual_outputs) = {
            // Need to fetch outputs specifically, current get_compute_node_details returns inputs.
            // Let's rely on update_express logic or fetching properly.
            // Actually get_compute_node_details implementation fetches "type" and "inputs".
            // We need to fetch "outputs" too.
            // Let's update get_compute_node_details signature or logic first?
            // For now, let's fetch the item directly here to be safe.
            let resp = self.client.get_item()
                .table_name(&self.table_name)
                .key("pk", AttributeValue::S(Self::pk_compute(&id)))
                .key("sk", AttributeValue::S(Self::sk_meta()))
                .send()
                .await?;
            if let Some(item) = resp.item {
                 let outputs = item.get("outputs").and_then(|av| av.as_l().ok())
                    .map(|l| l.iter().filter_map(|av| av.as_s().ok().cloned()).collect())
                    .unwrap_or_default();
                 (String::new(), outputs)
            } else {
                (String::new(), vec![])
            }
        };

        // 2. TransactWrite: Update Compute Node + Mark Orphans
        let mut transaction = Vec::new();

        // Update Compute Node (Set outputs, Remove dirty)
        let output_attrs: Vec<AttributeValue> = outputs.iter().map(|i| AttributeValue::S(i.clone())).collect();
        // DynamoDB Transactions don't support UpdateItem with complex UpdateExpressions easily mixed with others on same item? 
        // Actually they do.
        let update_compute = aws_sdk_dynamodb::types::Update::builder()
            .table_name(&self.table_name)
            .key("pk", AttributeValue::S(Self::pk_compute(&id)))
            .key("sk", AttributeValue::S(Self::sk_meta()))
            .update_expression("SET outputs = :o REMOVE gsi1pk, gsi1sk")
            .expression_attribute_values(":o", AttributeValue::L(output_attrs))
            .build()
            .unwrap();
        transaction.push(TransactWriteItem::builder().update(update_compute).build());

        // Mark Orphans
        for old in old_actual_outputs {
            if !outputs.contains(&old) {
                 let update_orphan = aws_sdk_dynamodb::types::Update::builder()
                    .table_name(&self.table_name)
                    .key("pk", AttributeValue::S(Self::pk_artifact(&old)))
                    .key("sk", AttributeValue::S(Self::sk_meta()))
                    .update_expression("SET gsi1pk = :pk, gsi1sk = :sk")
                    .expression_attribute_values(":pk", AttributeValue::S(Self::gsi1_orphan_pk()))
                    .expression_attribute_values(":sk", AttributeValue::S(Self::gsi1_artifact_sk(&old)))
                    .build()
                    .unwrap();
                transaction.push(TransactWriteItem::builder().update(update_orphan).build());
            }
        }
        
        self.client.transact_write_items()
            .set_transact_items(Some(transaction))
            .send()
            .await?;
        
        Ok(())
    }

    /// Deletes a compute node and its input edges.
    /// Marks output artifacts as STATUS#ORPHAN.
    pub async fn remove_compute_node(&self, id: ComputeNodeId) -> Result<(), aws_sdk_dynamodb::Error> {
        // 1. Fetch details to find inputs (to delete edges) and outputs (to orphan)
        let resp = self.client.get_item()
                .table_name(&self.table_name)
                .key("pk", AttributeValue::S(Self::pk_compute(&id)))
                .key("sk", AttributeValue::S(Self::sk_meta()))
                .send()
                .await?;
        
        let (inputs, outputs) = if let Some(item) = resp.item {
             let inputs: Vec<String> = item.get("inputs").and_then(|av| av.as_l().ok())
                .map(|l| l.iter().filter_map(|av| av.as_s().ok().cloned()).collect())
                .unwrap_or_default();
             let outputs: Vec<String> = item.get("outputs").and_then(|av| av.as_l().ok())
                .map(|l| l.iter().filter_map(|av| av.as_s().ok().cloned()).collect())
                .unwrap_or_default();
             (inputs, outputs)
        } else {
            return Ok(()); // Already gone?
        };

        let mut transaction = Vec::new();

        // 1. Delete Compute Node
        transaction.push(TransactWriteItem::builder().delete(
            aws_sdk_dynamodb::types::Delete::builder()
                .table_name(&self.table_name)
                .key("pk", AttributeValue::S(Self::pk_compute(&id)))
                .key("sk", AttributeValue::S(Self::sk_meta()))
                .build()
                .unwrap()
        ).build());

        // 2. Delete Input Edges
        for input in inputs {
            transaction.push(TransactWriteItem::builder().delete(
                aws_sdk_dynamodb::types::Delete::builder()
                    .table_name(&self.table_name)
                    .key("pk", AttributeValue::S(Self::pk_artifact(&input)))
                    .key("sk", AttributeValue::S(Self::sk_edge_compute(&id)))
                    .build()
                    .unwrap()
            ).build());
        }

        // 3. Mark Outputs as ORPHAN
        for output in outputs {
             let update_orphan = aws_sdk_dynamodb::types::Update::builder()
                .table_name(&self.table_name)
                .key("pk", AttributeValue::S(Self::pk_artifact(&output)))
                .key("sk", AttributeValue::S(Self::sk_meta()))
                .update_expression("SET gsi1pk = :pk, gsi1sk = :sk")
                .expression_attribute_values(":pk", AttributeValue::S(Self::gsi1_orphan_pk()))
                .expression_attribute_values(":sk", AttributeValue::S(Self::gsi1_artifact_sk(&output)))
                .build()
                .unwrap();
            transaction.push(TransactWriteItem::builder().update(update_orphan).build());
        }

        self.client.transact_write_items()
            .set_transact_items(Some(transaction))
            .send()
            .await?;

        Ok(())
    }

    /// Scans STATUS#ORPHAN index and deletes artifacts if they have no downstream consumers.
    /// Returns count of deleted items.
    pub async fn cleanup_orphans(&self) -> Result<usize, aws_sdk_dynamodb::Error> {
        let mut deleted_count = 0;
        
        // 1. Query Orphans from GSI
        let resp = self.client.query()
            .table_name(&self.table_name)
            .index_name("gsi1")
            .key_condition_expression("gsi1pk = :pk")
            .expression_attribute_values(":pk", AttributeValue::S(Self::gsi1_orphan_pk()))
            .send()
            .await?;

        if let Some(items) = resp.items {
            for item in items {
                // Ensure we have the PK (Artifact ID)
                // GSI projection ALL includes PK.
                // In STD, main table PK is "ARTIFACT#<id>".
                if let Some(pk_attr) = item.get("pk") {
                    if let Ok(pk) = pk_attr.as_s() {
                         let _artifact_id = pk.trim_start_matches("ARTIFACT#").to_string();
                         
                         // 2. Check downstream edges
                         // We have methods for this, but they are async stream.
                         // Let's just do a quick count query.
                         let edges = self.client.query()
                            .table_name(&self.table_name)
                            .key_condition_expression("pk = :pk AND begins_with(sk, :prefix)")
                            .expression_attribute_values(":pk", AttributeValue::S(pk.clone()))
                            .expression_attribute_values(":prefix", AttributeValue::S("COMPUTE#".to_string()))
                            .select(aws_sdk_dynamodb::types::Select::Count)
                            .send()
                            .await?;
                        
                        if edges.count == 0 {
                            // 3. Safe to Delete
                            self.client.delete_item()
                                .table_name(&self.table_name)
                                .key("pk", AttributeValue::S(pk.clone()))
                                .key("sk", AttributeValue::S(Self::sk_meta()))
                                .send()
                                .await?;
                            deleted_count += 1;
                        }
                    }
                }
            }
        }
        Ok(deleted_count)
    }

    /// Finds all compute nodes that use this artifact as an input.
    pub fn get_downstream_compute_nodes(&self, artifact_id: ArtifactId) -> Pin<Box<dyn Stream<Item = Result<ComputeNodeId, aws_sdk_dynamodb::Error>> + Send>> {
        let pk = Self::pk_artifact(&artifact_id);
        let client = self.client.clone();
        let table = self.table_name.clone();

        // Simple non-paginated implementation for MVP
        let stream = async_stream::try_stream! {
            let resp = client.query()
                .table_name(table)
                .key_condition_expression("pk = :pk AND begins_with(sk, :prefix)")
                .expression_attribute_values(":pk", AttributeValue::S(pk))
                .expression_attribute_values(":prefix", AttributeValue::S("COMPUTE#".to_string()))
                .send()
                .await?;

            if let Some(items) = resp.items {
                for item in items {
                    if let Some(sk) = item.get("sk") {
                         if let Ok(s) = sk.as_s() {
                             // sk is COMPUTE#<ID>
                             let id = s.trim_start_matches("COMPUTE#").to_string();
                             yield id;
                         }
                    }
                }
            }
        };
        Box::pin(stream)
    }

    /// Returns (node_type, inputs).
    pub async fn get_compute_node_details(&self, id: ComputeNodeId) -> Result<(String, Vec<ArtifactId>), aws_sdk_dynamodb::Error> {
        let resp = self.client.get_item()
            .table_name(&self.table_name)
            .key("pk", AttributeValue::S(Self::pk_compute(&id)))
            .key("sk", AttributeValue::S(Self::sk_meta()))
            .send()
            .await?;

        if let Some(item) = resp.item {
            let node_type = item.get("type").and_then(|av| av.as_s().ok()).cloned().unwrap_or_default();
            let inputs = item.get("inputs").and_then(|av| av.as_l().ok())
                .map(|l| l.iter().filter_map(|av| av.as_s().ok().cloned()).collect())
                .unwrap_or_default();
            
            Ok((node_type, inputs))
        } else {
            // Or return error if not found? For now empty default or error logic
             Ok(("".to_string(), vec![]))
             // In real app, might want specific NotFound error
        }
    }

    /// Returns all compute nodes that need execution.
    pub fn get_dirty_compute_nodes(&self) -> Pin<Box<dyn Stream<Item = Result<ComputeNodeId, aws_sdk_dynamodb::Error>> + Send>> {
        let client = self.client.clone();
        let table = self.table_name.clone();

        let stream = async_stream::try_stream! {
            let resp = client.query()
                .table_name(table)
                .index_name("gsi1")
                .key_condition_expression("gsi1pk = :pk")
                .expression_attribute_values(":pk", AttributeValue::S(Self::gsi1_dirty_pk()))
                .send()
                .await?;

            if let Some(items) = resp.items {
                for item in items {
                    if let Some(sk) = item.get("gsi1sk") {
                        if let Ok(s) = sk.as_s() {
                             // sk is COMPUTE#<ID>
                            let id = s.trim_start_matches("COMPUTE#").to_string();
                            yield id;
                        }
                    }
                }
            }
        };
        Box::pin(stream)
    }
}
